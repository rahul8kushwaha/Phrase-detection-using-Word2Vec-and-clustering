{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjHI2Q5IQ230"
      },
      "source": [
        "## *Phrase Detection algorithm*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FY15NssQ9Ew"
      },
      "source": [
        "##The basic idea is to get word2vec skip gram model embedding using a big text data (here I have used a a gutenberg novel).\n",
        "##Since the above embeddings capture both symantic and syntactic relation between word we can use the similarity between these vector to form a cluster with a different clustering approach.\n",
        "##Once we get the embeddings if a new sentence is fed to the model the model converts the word to embeddings. Initially if there are n word the number of clusters are n.\n",
        "##Then we can compare similarities between the clusters using euclidean distance, cosine similarity, etc calculated between newly obtained vectors using weights as shown below for the consecutive clusters in the sentence. This done to maintain a sequence. The two consecutive clusters having maximum similarity are merged together to form one cluster.\n",
        "##This process is repeated until we get a single cluster. This would give us a tree like structure.\n",
        "##Appropriate phrases can be selected from the above tree at appropriate level since the phrases are not unique.\n",
        "##The approach is similar to agglomerative hierarchical clustering\n",
        "## Weights for left cluster=[n.....,4,3,2,1]/(nx(n+1)/2)\n",
        "## Weights for right cluster=[1,2,3,4,.....n]/(nx(n+1)/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_pYN9a4c6ek"
      },
      "source": [
        "Data Reading and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U3rkHhE0kcQ",
        "outputId": "f533db63-7a0f-4d99-906e-e5c6b160015f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snxN6V7ESNZN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import spatial\n",
        "data=open('/content/drive/MyDrive/Datasets/66568-0.txt').read()                        #write the location of the text file here\n",
        "for x in list('!\"#$%&\\'()*+,-/:;<=>?@[\\]^_`{|}~'):\n",
        "  data=data.replace(x,'')\n",
        "data=data.replace('\\n','.')\n",
        "data=data.split('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBZzVwVydVhA"
      },
      "source": [
        "Training of word2vec skip gram model using window size 5 and minimum count to be 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag8sQ7yXe9Gw"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data)):\n",
        "  data[i]=data[i].split()\n",
        "model=Word2Vec(data,size=100,min_count=1,sg=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RgRQfbAdZ8c"
      },
      "source": [
        "Embedding a new sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urKTWSNThPfi"
      },
      "outputs": [],
      "source": [
        "t='There was no one to tell her that Philibert had once wanted to marry Bianca'.split()  #Type the sentence manually here\n",
        "#t=input().split()                                                                       #Or use this line of code for input\n",
        "embedding=[]\n",
        "for word in t:\n",
        "  embedding.append(model.wv.word_vec(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0xUc182dfoI"
      },
      "source": [
        "Clustering the words to form phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6y30CVUnJuJ",
        "outputId": "61a75ca6-db70-4ac7-912c-ea41ae6314d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There was no one to tell her that Philibert had once wanted to marry Bianca\n",
            "\n",
            "There was no one to tell her that<-------->Philibert had once wanted to marry Bianca\n",
            "\n",
            "There was no one to tell her that<-------->Philibert had once<-------->wanted to marry Bianca\n",
            "\n",
            "There was no<-------->one to tell her that<-------->Philibert had once<-------->wanted to marry Bianca\n",
            "\n",
            "There was no<-------->one to<-------->tell her that<-------->Philibert had once<-------->wanted to marry Bianca\n",
            "\n",
            "There was no<-------->one to<-------->tell her that<-------->Philibert had<-------->once<-------->wanted to marry Bianca\n",
            "\n",
            "There was no<-------->one to<-------->tell her that<-------->Philibert had<-------->once<-------->wanted to marry<-------->Bianca\n",
            "\n",
            "There was no<-------->one to<-------->tell her<-------->that<-------->Philibert had<-------->once<-------->wanted to marry<-------->Bianca\n",
            "\n",
            "There was<-------->no<-------->one to<-------->tell her<-------->that<-------->Philibert had<-------->once<-------->wanted to marry<-------->Bianca\n",
            "\n",
            "There was<-------->no<-------->one to<-------->tell her<-------->that<-------->Philibert had<-------->once<-------->wanted<-------->to marry<-------->Bianca\n",
            "\n",
            "There was<-------->no<-------->one to<-------->tell her<-------->that<-------->Philibert had<-------->once<-------->wanted<-------->to<-------->marry<-------->Bianca\n",
            "\n",
            "There was<-------->no<-------->one<-------->to<-------->tell her<-------->that<-------->Philibert had<-------->once<-------->wanted<-------->to<-------->marry<-------->Bianca\n",
            "\n",
            "There was<-------->no<-------->one<-------->to<-------->tell<-------->her<-------->that<-------->Philibert had<-------->once<-------->wanted<-------->to<-------->marry<-------->Bianca\n",
            "\n",
            "There was<-------->no<-------->one<-------->to<-------->tell<-------->her<-------->that<-------->Philibert<-------->had<-------->once<-------->wanted<-------->to<-------->marry<-------->Bianca\n",
            "\n",
            "There<-------->was<-------->no<-------->one<-------->to<-------->tell<-------->her<-------->that<-------->Philibert<-------->had<-------->once<-------->wanted<-------->to<-------->marry<-------->Bianca\n",
            "\n"
          ]
        }
      ],
      "source": [
        "new_embedding=[[np.array(element)] for element in embedding]\n",
        "new_cluster_word=[[element] for element in t.copy()]\n",
        "clustering=[new_cluster_word.copy()]\n",
        "def cluster_weight_left(vectors):\n",
        "  n=len(vectors)\n",
        "  weights=np.array([i*2/(n*(n+1)) for i in range(1,n+1)])\n",
        "  final_vector=np.dot(np.array(vectors).T,weights)\n",
        "  return final_vector\n",
        "def cluster_weight_right(vectors):\n",
        "  n=len(vectors)\n",
        "  weights=np.array([(n-i+1)*2/(n*(n+1)) for i in range(1,n+1)])\n",
        "  final_vector=np.dot(np.array(vectors).T,weights)\n",
        "  return final_vector\n",
        "j=1\n",
        "similarity=[]\n",
        "while len(new_embedding)>1:\n",
        "  sim=[]\n",
        "  for i in range(len(new_embedding)-1):\n",
        "    #sim.append(np.linalg.norm(new_embedding[i][-1]-new_embedding[i+1][0]))                                                       #cluster using nearest inner two words using Euclidean Distance\n",
        "    #sim.append(np.linalg.norm(sum(new_embedding[i])/len(new_embedding[i])-sum(new_embedding[i+1])/len(new_embedding[i+1])))      #cluster using average of vector in the two consecutive clusters using Euclidean Distance\n",
        "    sim.append(np.linalg.norm(cluster_weight_left(new_embedding[i])-cluster_weight_right(new_embedding[i+1])))                    #cluster using weighted average of vector in the two consecutive clusters using Euclidean Distance\n",
        "    #sim.append(spatial.distance.cosine(cluster_weight_left(new_embedding[i]),cluster_weight_right(new_embedding[i+1])))          #cluster using weighted average of vector in the two consecutive clusters using Euclidean Distance\n",
        "    #sim.append(spatial.distance.cosine(sum(new_embedding[i])/len(new_embedding[i]),sum(new_embedding[i+1])/len(new_embedding[i+1]))) #cluster using average of vector in the two consecutive clusters using Euclidean Distance\n",
        "  sim=np.array(sim)\n",
        "  pos=np.argmax(sim)\n",
        "  new_embedding[pos]+=new_embedding[pos+1]\n",
        "  new_embedding.pop(pos+1)\n",
        "  new_cluster_word[pos]=new_cluster_word[pos]+new_cluster_word[pos+1].copy()\n",
        "  new_cluster_word.pop(pos+1)\n",
        "  clustering.insert(0,new_cluster_word.copy())\n",
        "  similarity.insert(0,sum(sim)/len(sim))\n",
        "  j+=1\n",
        "for line in clustering:\n",
        "  for element in line[:-1]:\n",
        "    print(' '.join(element),end='<-------->')\n",
        "  print(' '.join(line[-1]),end='\\n'*2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_uUwIu7zP5A"
      },
      "source": [
        "From the above tree line structure approriate level will give us the appropriate phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WByh3CtZy4s-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwGpqsSHy4vy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnx0eov1y4ye"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVgvVhEjy42N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka2Y1j7qy451"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo2hAFPVy47E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PRGNcVkWoTlu",
        "outputId": "d1195531-6be5-4171-9b07-6e156a9513ef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport matplotlib.pyplot as plt\\nplt.plot(similarity)\\nplt.show\\n'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(similarity)\n",
        "plt.show\n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "04UCMYoVwCqn",
        "outputId": "8d115c4c-7099-4c95-b281-4c39c38cf37d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsim_slope=np.array([similarity[i+1]-similarity[i] for i in range(len(similarity)-1)])\\nplt.plot(sim_slope)\\nplt.show\\n'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "sim_slope=np.array([similarity[i+1]-similarity[i] for i in range(len(similarity)-1)])\n",
        "plt.plot(sim_slope)\n",
        "plt.show\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OdT3BlYqwi_G",
        "outputId": "f2a29532-6733-46a0-b501-bfcdd2a63e5c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nbest_level=sim_slope[1:].argmin()-2\\nfor element in clustering[best_level]:\\n  print(' '.join(element),end='<-------->')\\n\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "best_level=sim_slope[1:].argmin()-2\n",
        "for element in clustering[best_level]:\n",
        "  print(' '.join(element),end='<-------->')\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP_assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
